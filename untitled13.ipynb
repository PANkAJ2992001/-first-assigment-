{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1TvuKD59+bmXFKKRthRuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PANkAJ2992001/-first-assigment-/blob/main/untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnTQjNfl-RML"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "              Theoretical\n",
        "\n",
        "1. What is K-Nearest Neighbors (KNN) and how does it work?\n",
        "ans. K-Nearest Neighbors (KNN) is a supervised learning algorithm that predicts the class label or target value of a new data point by finding its K nearest neighbors in the training data and using their labels or values to make a prediction.\n",
        "\n",
        "2. What is the difference between KNN Classification and KNN Regression?\n",
        "ans. The main difference between KNN Classification and KNN Regression is:\n",
        "\n",
        "KNN Classification:\n",
        "\n",
        "- Predicts a categorical class label\n",
        "- Uses majority voting to determine the class label\n",
        "- Suitable for problems with discrete outcomes (e.g., spam/not spam emails)\n",
        "\n",
        "KNN Regression:\n",
        "\n",
        "- Predicts a continuous target value\n",
        "- Uses average or weighted average of neighboring values to make predictions\n",
        "- Suitable for problems with continuous outcomes (e.g., predicting house prices)\n",
        "\n",
        "3. What is the role of the distance metric in KNN?\n",
        "ans. The distance metric in KNN measures the similarity or dissimilarity between data points. Its role is to:\n",
        "\n",
        "1. Calculate the distance between the new data point and all training data points.\n",
        "2. Determine the K nearest neighbors.\n",
        "3. Influence the prediction outcome.\n",
        "\n",
        "4. What is the Curse of Dimensionality in KNN?\n",
        "ans. The Curse of Dimensionality in KNN refers to the decrease in accuracy and increase in computational cost as the number of features (dimensions) increases, making it harder to find meaningful nearest neighbors.\n",
        "\n",
        "5.  How can we choose the best value of K in KNN?\n",
        "ans. To choose the best value of K in KNN:\n",
        "\n",
        "1. Cross-validation: Split data into training and validation sets, and try different K values.\n",
        "2. Grid search: Try multiple K values and evaluate performance using metrics like accuracy or mean squared error.\n",
        "3. Plotting: Visualize performance metrics against different K values to identify the optimal range.\n",
        "4. Rule of thumb: Start with K = âˆšn, where n is the number of training samples.\n",
        "\n",
        "6. What are KD Tree and Ball Tree in KNN?\n",
        "ans. KD Tree and Ball Tree are data structures used in KNN to:\n",
        "\n",
        "1. Accelerate nearest neighbor search\n",
        "2. Reduce computational complexity\n",
        "3. Improve search efficiency\n",
        "\n",
        "KD Tree:\n",
        "\n",
        "- Divides data into smaller subsets based on median values\n",
        "- Constructs a balanced tree for efficient search\n",
        "\n",
        "Ball Tree:\n",
        "\n",
        "- Clusters data points into a tree-like structure\n",
        "- Represents clusters as ball-shaped regions in feature space\n",
        "\n",
        "7. When should you use KD Tree vs. Ball Tree?\n",
        "ans. Here's a brief guide:\n",
        "\n",
        "KD Tree\n",
        "1. High-dimensional data: KD Trees perform well with many features (dimensions).\n",
        "2. Euclidean distance: KD Trees are optimized for Euclidean distance metric.\n",
        "3. Static datasets: KD Trees are suitable for datasets that don't change often.\n",
        "\n",
        "Ball Tree\n",
        "1. Low- to medium-dimensional data: Ball Trees work well with fewer features.\n",
        "2. Non-Euclidean distances: Ball Trees support various distance metrics, including non-Euclidean ones.\n",
        "3. Dynamic datasets: Ball Trees can handle datasets that change frequently.\n",
        "\n",
        "8. What are the disadvantages of KNN?\n",
        "ans. Here are the main disadvantages of KNN:\n",
        "\n",
        "1. Computational complexity: KNN can be slow for large datasets.\n",
        "2. Sensitive to noise: Noisy data can negatively impact KNN's performance.\n",
        "3. Difficulty in choosing K: Selecting the optimal value of K can be challenging.\n",
        "4. Curse of dimensionality: KNN's performance degrades with high-dimensional data.\n",
        "5. Not suitable for complex relationships: KNN struggles with complex, non-linear relationships.\n",
        "6. Requires feature scaling: KNN requires features to be scaled to ensure equal importance.\n",
        "7. Vulnerable to outliers: Outliers can significantly impact KNN's predictions.\n",
        "\n",
        "9. How does feature scaling affect KNN?\n",
        "ans. Feature scaling ensures all features are on the same scale, preventing features with large ranges from dominating distance calculations and improving KNN's accuracy and interpretability.\n",
        "\n",
        "10. What is PCA (Principal Component Analysis)?\n",
        "ans. Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into lower-dimensional data while retaining most of the information.\n",
        "\n",
        "11. How does PCA work?\n",
        "ans. PCA works by:\n",
        "\n",
        "1. Standardizing data\n",
        "2. Computing covariance matrix\n",
        "3. Finding eigenvectors and eigenvalues\n",
        "4. Selecting top eigenvectors (principal components)\n",
        "5. Transforming data onto new coordinate system\n",
        "\n",
        "12. What is the geometric intuition behind PCA?\n",
        "ans. Geometric intuition behind PCA:\n",
        "\n",
        "PCA finds directions of maximum variance in the data cloud, stretching and aligning it to its natural directions, and flattening it onto lower-dimensional space.\n",
        "\n",
        "13. What is the difference between Feature Selection and Feature Extraction?\n",
        "ans. Here's the difference:\n",
        "\n",
        "Feature Selection\n",
        "1. Selects a subset: Chooses the most relevant features from the existing set.\n",
        "2. Retains original features: Selected features remain unchanged.\n",
        "3. Reduces dimensionality: Eliminates irrelevant or redundant features.\n",
        "\n",
        "Feature Extraction\n",
        "1. Transforms original features: Creates new features from the existing set.\n",
        "2. Combines or transforms: Uses techniques like PCA, t-SNE, or autoencoders to create new features.\n",
        "3. Reduces dimensionality: Represents the data in a lower-dimensional space.\n",
        "\n",
        "14. What are Eigenvalues and Eigenvectors in PCA?\n",
        "ans. In PCA:\n",
        "\n",
        "- Eigenvalues: Represent variance and importance of principal components.\n",
        "- Eigenvectors: Represent directions of new axes (principal components) and are orthogonal to each other.\n",
        "\n",
        "15. How do you decide the number of components to keep in PCA?\n",
        "ans. Here are the answers:\n",
        "\n",
        "1. Visual Inspection: Look for \"elbow\" point in explained variance ratio plot.\n",
        "2. Cumulative Variance: Choose components capturing 95% of total variance.\n",
        "3. Cross-Validation: Evaluate model performance with different component numbers.\n",
        "4. Scree Plot: Look for significant drop-off in eigenvalues.\n",
        "5. Domain Knowledge: Use expertise to determine meaningful component number.\n",
        "\n",
        "16. Can PCA be used for classification?\n",
        "ans. PCA can be used as a preprocessing step for classification algorithms, to:\n",
        "\n",
        "- Reduce dimensionality\n",
        "- Improve data quality\n",
        "- Enhance classifier performance\n",
        "\n",
        "17. What are the limitations of PCA?\n",
        "ans. Limitations of PCA:\n",
        "\n",
        "1. Assumes linearity: PCA assumes linear relationships between features.\n",
        "2. Sensitive to scaling: PCA is sensitive to feature scaling.\n",
        "3. Not robust to outliers: PCA can be affected by outliers.\n",
        "4. Loss of information: PCA discards some information during dimensionality reduction.\n",
        "5. Not suitable for non-Gaussian data: PCA assumes Gaussian distribution of data.\n",
        "\n",
        "18. How do KNN and PCA complement each other?\n",
        "ans. KNN and PCA complement each other in:\n",
        "\n",
        "1. Dimensionality reduction: PCA reduces dimensions, improving KNN's performance.\n",
        "2. Noise reduction: PCA reduces noise, making KNN more robust.\n",
        "3. Feature selection: PCA selects relevant features, improving KNN's accuracy.\n",
        "4. Improved classification: PCA's reduced features improve KNN's classification performance.\n",
        "\n",
        "19.  How does KNN handle missing values in a dataset?\n",
        "ans. KNN handles missing values in a dataset by:\n",
        "\n",
        "1. Ignoring samples: Ignoring samples with missing values.\n",
        "2. Mean/Median imputation: Replacing missing values with mean/median of the feature.\n",
        "3. KNN imputation: Using KNN to impute missing values based on similarities with other samples.\n",
        "4. Weighted distance: Using weighted distance metrics that ignore missing value\n",
        "\n",
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "ans. Key differences:\n",
        "\n",
        "1. Goal: PCA (dimensionality reduction), LDA (classification).\n",
        "2. Objective function: PCA (maximize variance), LDA (maximize class separation).\n",
        "3. Transformation: PCA (orthogonal), LDA (not necessarily orthogonal).\n",
        "4. Assumptions: PCA (Gaussian distribution), LDA (Gaussian distribution, equal covariance).\n",
        "5. Number of components: PCA (any number), LDA (at most c-1, where c is number of classes)\n",
        "\n",
        "\n",
        "                         Practical\n",
        "\n",
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy?\n",
        "ans. Here's some sample Python code:\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Regressor\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate model using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "23.  Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier using Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(\"Euclidean Distance Accuracy:\", accuracy_euclidean)\n",
        "\n",
        "# Train KNN Classifier using Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(\"Manhattan Distance Accuracy:\", accuracy_manhattan)\n",
        "\n",
        "24. Train a KNN Classifier with different values of K and visualize decision boundaried?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a meshgrid of points\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Train KNN Classifier with different values of K\n",
        "for k in [3, 5, 10]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=20, edgecolor='k')\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolor='k')\n",
        "    plt.title(f\"KNN Classifier with K={k}\")\n",
        "    plt.show()\n",
        "\n",
        "25.  Apply Feature Scaling before training a KNN model and compare results with unscaled data?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN model without feature scaling\n",
        "knn_without_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_without_scaling.fit(X_train, y_train)\n",
        "y_pred_without_scaling = knn_without_scaling.predict(X_test)\n",
        "print(\"Without Feature Scaling:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_without_scaling))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_without_scaling))\n",
        "\n",
        "# Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN model with feature scaling\n",
        "knn_with_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = knn_with_scaling.predict(X_test_scaled)\n",
        "print(\"\\nWith Feature Scaling:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_with_scaling))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n",
        "\n",
        "26. 5 Train a PCA model on synthetic data and print the explained variance ratio for each component?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 5)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA(n_components=5)\n",
        "pca.fit(X)\n",
        "\n",
        "# Print explained variance ratio for each component\n",
        "print(\"Explained Variance Ratio:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Component {i+1}: {ratio:.2f}\")\n",
        "\n",
        "# Print cumulative sum of explained variance ratio\n",
        "print(\"\\nCumulative Sum of Explained Variance Ratio:\")\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA5?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier without PCA\n",
        "knn_without_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_without_pca.fit(X_train, y_train)\n",
        "y_pred_without_pca = knn_without_pca.predict(X_test)\n",
        "accuracy_without_pca = accuracy_score(y_test, y_pred_without_pca)\n",
        "print(\"Accuracy without PCA:\", accuracy_without_pca)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier with PCA\n",
        "knn_with_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_with_pca.fit(X_train_pca, y_train)\n",
        "y_pred_with_pca = knn_with_pca.predict(X_test_pca)\n",
        "accuracy_with_pca = accuracy_score(y_test, y_pred_with_pca)\n",
        "print(\"Accuracy with PCA:\", accuracy_with_pca)\n",
        "\n",
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter tuning space\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Perform hyperparameter tuning using GridSearchCV\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters and corresponding accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Train KNN Classifier with best hyperparameters and evaluate on test set\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred = best_knn.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "29. Train a KNN Classifier and check the number of misclassified samples?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Check the number of misclassified samples\n",
        "misclassified_samples = np.where(y_test != y_pred)[0]\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified_samples))\n",
        "\n",
        "30. Train a PCA model and visualize the cumulative explained variance?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA().fit(X)\n",
        "\n",
        "# Get explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Get cumulative sum of explained variance ratio\n",
        "cumulative_sum = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(cumulative_sum, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare\n",
        "accuracy?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier with uniform weights\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "print(\"Uniform Weights Accuracy:\", accuracy_uniform)\n",
        "\n",
        "# Train KNN Classifier with distance weights\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test)\n",
        "accuracy_distance = accuracy_score(y_test, y_pred_distance)\n",
        "print(\"Distance Weights Accuracy:\", accuracy_distance)\n",
        "\n",
        "32. Train a KNN Regressor and analyze the effect of different K values on performance?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different K values\n",
        "k_values = np.arange(1, 21)\n",
        "\n",
        "# Initialize list to store MSE values\n",
        "mse_values = []\n",
        "\n",
        "# Train KNN Regressor with different K values\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "# Plot MSE values against K values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_values, mse_values, marker='o')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Effect of K Value on KNN Regressor Performance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "33. Implement KNN Imputation for handling missing values in a dataset?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample dataset with missing values\n",
        "data = {'Feature1': [1, 2, np.nan, 4, 5],\n",
        "        'Feature2': [np.nan, 2, 3, 4, 5],\n",
        "        'Feature3': [1, 2, 3, 4, np.nan]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the original dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df)\n",
        "\n",
        "# Implement KNN Imputation\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "imputed_data = imputer.fit_transform(df)\n",
        "\n",
        "# Convert imputed data back to DataFrame\n",
        "imputed_df = pd.DataFrame(imputed_data, columns=df.columns)\n",
        "\n",
        "# Print the imputed dataset\n",
        "print(\"\\nImputed Dataset:\")\n",
        "print(imputed_df)\n",
        "\n",
        "34. Train a PCA model and visualize the data projection onto the first two principal components?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot data projection onto first two principal components\n",
        "plt.figure(figsize=(8, 6))\n",
        "for c, i, target_name in zip(\"rgb\", [0, 1, 2], iris.target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=c, label=target_name)\n",
        "plt.title(\"PCA of IRIS dataset\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance?\n",
        "ans. To compare the performance of KD Tree and Ball Tree algorithms in a KNN Classifier:\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "knn_kd_tree = KNeighborsClassifier(algorithm='kd_tree')\n",
        "knn_ball_tree = KNeighborsClassifier(algorithm='ball_tree')\n",
        "\n",
        "knn_kd_tree.fit(X_train, y_train)\n",
        "knn_ball_tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_kd_tree = knn_kd_tree.predict(X_test)\n",
        "y_pred_ball_tree = knn_ball_tree.predict(X_test)\n",
        "\n",
        "print(\"KD Tree Accuracy:\", accuracy_score(y_test, y_pred_kd_tree))\n",
        "print(\"Ball Tree Accuracy:\", accuracy_score(y_test, y_pred_ball_tree))\n",
        "\n",
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA().fit(X)\n",
        "\n",
        "# Get explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot Scree plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "37.  Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "38. Train a PCA model and analyze the effect of different numbers of components on accuracy?\n",
        "ans. from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Define PCA models with different numbers of components\n",
        "components = [1, 2, 3, 4]\n",
        "accuracy_values = []\n",
        "\n",
        "for n in components:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    \n",
        "    # Train KNN Classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_values.append(accuracy)\n",
        "\n",
        "# Plot accuracy values\n",
        "plt.plot(components, accuracy_values, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy?\n",
        "ans. from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different leaf_size values\n",
        "leaf_sizes = [10, 20, 30, 40, 50]\n",
        "\n",
        "# Initialize list to store accuracy values\n",
        "accuracy_values = []\n",
        "\n",
        "# Train KNN Classifier with different leaf_size values\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf_size)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_values.append(accuracy)\n",
        "\n",
        "# Print accuracy values for different leaf_size values\n",
        "for i, accuracy in enumerate(accuracy_values):\n",
        "    print(f\"Accuracy with leaf_size={leaf_sizes[i]}: {accuracy:.2f}\")\n",
        "\n",
        "40. Train a PCA model and visualize how data points are transformed before and after PCA?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "sc = StandardScaler()\n",
        "X_std = sc.fit_transform(X)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Plot original data points\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_std[:, 0], X_std[:, 1], c=y)\n",
        "plt.title(\"Original Data Points\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "# Plot transformed data points\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.title(\"Transformed Data Points (PCA)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.show()\n",
        "\n",
        "41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different distance metrics\n",
        "metrics = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
        "\n",
        "# Initialize list to store prediction errors\n",
        "errors = []\n",
        "\n",
        "# Train KNN Regressor with different distance metrics\n",
        "for metric in metrics:\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    error = mean_squared_error(y_test, y_pred)\n",
        "    errors.append(error)\n",
        "\n",
        "# Print prediction errors for different distance metrics\n",
        "for i, error in enumerate(errors):\n",
        "    print(f\"Error using {metrics[i]} metric: {error:.2f}\")\n",
        "\n",
        "43.  Train a KNN Classifier and evaluate using ROC-AUC score?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Convert multi-class problem to binary classification\n",
        "y_binary = (y > 4).astype(int)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_proba = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate using ROC-AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"ROC-AUC Score:\", auc)\n",
        "\n",
        "44. Train a PCA model and visualize the variance captured by each principal component?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA().fit(X)\n",
        "\n",
        "# Get explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot variance captured by each principal component\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(explained_variance_ratio, marker='o')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Captured')\n",
        "plt.title('Variance Captured by Each Principal Component')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot cumulative variance captured\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.cumsum(explained_variance_ratio), marker='o')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Cumulative Variance Captured')\n",
        "plt.title('Cumulative Variance Captured by Each Principal Component')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "45. Train a KNN Classifier and perform feature selection before training?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform feature selection using chi2 statistic\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier on selected features\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = knn.predict(X_test_selected)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "46. Train a PCA model and visualize the data reconstruction error after reducing dimensions?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Train PCA model with different number of components\n",
        "components = np.arange(1, X.shape[1] + 1)\n",
        "reconstruction_errors = []\n",
        "\n",
        "for n in components:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_reduced = pca.fit_transform(X)\n",
        "    X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "    reconstruction_error = np.mean((X - X_reconstructed) ** 2)\n",
        "    reconstruction_errors.append(reconstruction_error)\n",
        "\n",
        "# Plot reconstruction error against number of components\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(components, reconstruction_errors, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('Reconstruction Error after Reducing Dimensions')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "47. Train a KNN Classifier and visualize the decision boundary?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use only 2 features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Create a meshgrid for visualization\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Make predictions on the meshgrid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Decision Boundary')\n",
        "plt.show()\n",
        "\n",
        "48.  Train a PCA model and analyze the effect of different numbers of components on data variance?\n",
        "ans. # Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Train PCA model with different numbers of components\n",
        "components = np.arange(1, X.shape[1] + 1)\n",
        "explained_variance_ratios = []\n",
        "\n",
        "for n in components:\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X)\n",
        "    explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
        "    explained_variance_ratios.append(explained_variance_ratio)\n",
        "\n",
        "# Plot explained variance ratio against number of components\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(components, explained_variance_ratios, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Effect of Number of Components on Data Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "                         "
      ],
      "metadata": {
        "id": "qyr-SN_g-8TZ"
      }
    }
  ]
}